# ğŸ“ æ•°æ®å­˜å‚¨è¯´æ˜

## 1. æ–‡ä»¶å­˜å‚¨ä½ç½®

### ğŸ“‚ ä¸Šä¼ çš„æ–‡ä»¶
**å­˜å‚¨è·¯å¾„**ï¼š`backend/uploads/`

```
backend/uploads/
â”œâ”€â”€ 7f23a1b4_é”€å”®æ•°æ®.xlsx        # æ–‡ä»¶å‘½åï¼š{file_id}_{åŸæ–‡ä»¶å}
â”œâ”€â”€ 9e45c2d8_ç”¨æˆ·æ•°æ®.csv
â””â”€â”€ a1b2c3d4_è´¢åŠ¡æŠ¥è¡¨.xlsx
```

**æ–‡ä»¶å‘½åè§„åˆ™**ï¼š
- `{UUIDå‰8ä½}_{åŸæ–‡ä»¶å}`
- ä¾‹å¦‚ï¼š`7f23a1b4_sales_data.xlsx`

**é…ç½®é¡¹** (`backend/config.py`):
```python
upload_dir: str = "./uploads"       # ä¸Šä¼ ç›®å½•
max_file_size: int = 104857600      # æœ€å¤§æ–‡ä»¶å¤§å° 100MB
```

---

## 2. æ•°æ®åº“å­˜å‚¨

### ğŸ“Š é£ä¹¦ç”¨æˆ·æ•°æ®åº“
**æ–‡ä»¶è·¯å¾„**ï¼š`backend/feishu_app.db` (SQLite)

åŒ…å«ä»¥ä¸‹è¡¨ï¼š

#### **users è¡¨** - ç”¨æˆ·ä¿¡æ¯
```sql
CREATE TABLE users (
    id INTEGER PRIMARY KEY,
    open_id TEXT UNIQUE NOT NULL,        -- é£ä¹¦ç”¨æˆ· ID
    name TEXT,                            -- ç”¨æˆ·å
    en_name TEXT,                         -- è‹±æ–‡å
    avatar_url TEXT,                      -- å¤´åƒ URL
    email TEXT,                           -- é‚®ç®±
    mobile TEXT,                          -- æ‰‹æœºå·
    department TEXT,                      -- éƒ¨é—¨
    is_active INTEGER DEFAULT 1,         -- æ˜¯å¦æ´»è·ƒ
    first_login_at TIMESTAMP,            -- é¦–æ¬¡ç™»å½•æ—¶é—´
    last_login_at TIMESTAMP,             -- æœ€åç™»å½•æ—¶é—´
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

**è®°å½•çš„æ•°æ®**ï¼š
- ç”¨æˆ·åŸºæœ¬ä¿¡æ¯ï¼ˆå§“åã€å¤´åƒã€é‚®ç®±ç­‰ï¼‰
- ç™»å½•æ—¶é—´ï¼ˆé¦–æ¬¡ã€æœ€è¿‘ï¼‰
- è´¦å·çŠ¶æ€

---

#### **analysis_history è¡¨** - åˆ†æå†å²
```sql
CREATE TABLE analysis_history (
    id INTEGER PRIMARY KEY,
    user_id TEXT NOT NULL,              -- ç”¨æˆ· open_id
    session_id TEXT,                    -- ä¼šè¯ ID
    query TEXT NOT NULL,                -- ç”¨æˆ·çš„åˆ†æè¯·æ±‚
    result TEXT,                        -- åˆ†æç»“æœï¼ˆJSONï¼‰
    result_type TEXT,                   -- ç»“æœç±»å‹ï¼ˆchart/table/textï¼‰
    file_name TEXT,                     -- ä½¿ç”¨çš„æ–‡ä»¶å
    chart_type TEXT,                    -- å›¾è¡¨ç±»å‹
    status TEXT DEFAULT 'pending',      -- çŠ¶æ€ï¼ˆsuccess/failed/pendingï¼‰
    error_message TEXT,                 -- é”™è¯¯ä¿¡æ¯
    execution_time REAL,                -- æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

**è®°å½•çš„æ•°æ®**ï¼š
- âœ… æ¯æ¬¡åˆ†æè¯·æ±‚å’Œç»“æœ
- âœ… æˆåŠŸ/å¤±è´¥çŠ¶æ€
- âœ… æ‰§è¡Œæ—¶é—´
- âœ… ç”Ÿæˆçš„å›¾è¡¨ç±»å‹

---

#### **user_sessions è¡¨** - ç”¨æˆ·ä¼šè¯
```sql
CREATE TABLE user_sessions (
    id INTEGER PRIMARY KEY,
    user_id TEXT NOT NULL,
    session_id TEXT UNIQUE NOT NULL,    -- Jupyter Session ID
    session_data TEXT,                  -- ä¼šè¯æ•°æ®ï¼ˆJSONï¼‰
    file_uploads TEXT,                  -- ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
    current_context TEXT,               -- å½“å‰ä¸Šä¸‹æ–‡
    is_active INTEGER DEFAULT 1,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    expires_at TIMESTAMP
);
```

**è®°å½•çš„æ•°æ®**ï¼š
- ç”¨æˆ·çš„æ´»åŠ¨ä¼šè¯
- ä¸Šä¼ çš„æ–‡ä»¶ä¿¡æ¯
- ä¼šè¯è¿‡æœŸæ—¶é—´

---

#### **file_uploads è¡¨** - æ–‡ä»¶ä¸Šä¼ è®°å½•
```sql
CREATE TABLE file_uploads (
    id INTEGER PRIMARY KEY,
    user_id TEXT NOT NULL,
    file_name TEXT NOT NULL,            -- åŸæ–‡ä»¶å
    file_path TEXT NOT NULL,            -- æœåŠ¡å™¨è·¯å¾„
    file_size INTEGER,                  -- æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    file_type TEXT,                     -- æ–‡ä»¶ç±»å‹ï¼ˆcsv/xlsxï¼‰
    sheets_count INTEGER,               -- å·¥ä½œè¡¨æ•°é‡
    total_rows INTEGER,                 -- æ€»è¡Œæ•°
    total_columns INTEGER,              -- æ€»åˆ—æ•°
    is_deleted INTEGER DEFAULT 0,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

**è®°å½•çš„æ•°æ®**ï¼š
- ç”¨æˆ·ä¸Šä¼ çš„æ¯ä¸ªæ–‡ä»¶
- æ–‡ä»¶å…ƒä¿¡æ¯ï¼ˆå¤§å°ã€è¡Œåˆ—æ•°ç­‰ï¼‰
- æ–‡ä»¶æ˜¯å¦è¢«åˆ é™¤

---

### ğŸ“Š åˆ†ææ•°æ®åº“
**æ–‡ä»¶è·¯å¾„**ï¼š`backend/data/analysis.db` (SQLite)

åŒ…å«è¡¨ï¼š
- `analysis_history` - Jupyter æ‰§è¡Œå†å²
- `session_records` - Jupyter Session è®°å½•

---

## 3. ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯

### ğŸ“ˆ ç»Ÿè®¡æ•°æ®æ¥æº

é€šè¿‡ `get_user_stats(user_id)` å‡½æ•°è·å–ï¼š

```python
stats = {
    "total_analyses": 15,        # æ€»åˆ†ææ¬¡æ•°
    "success_count": 12,         # æˆåŠŸæ¬¡æ•°
    "failed_count": 3,           # å¤±è´¥æ¬¡æ•°
    "total_files": 5,            # ä¸Šä¼ æ–‡ä»¶æ•°
    "active_sessions": 2,        # æ´»è·ƒä¼šè¯æ•°
    "avg_execution_time": 2.5,   # å¹³å‡æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
    "last_activity": "2025-11-03 16:30:00"  # æœ€åæ´»åŠ¨æ—¶é—´
}
```

**è®¡ç®—é€»è¾‘**ï¼š
```python
# backend/core/feishu_db.py

def get_user_stats(self, user_id):
    """è·å–ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯"""
    with self.get_connection() as conn:
        cursor = conn.cursor()
        
        # æ€»åˆ†ææ¬¡æ•°
        total = cursor.execute(
            "SELECT COUNT(*) FROM analysis_history WHERE user_id = ?",
            (user_id,)
        ).fetchone()[0]
        
        # æˆåŠŸæ¬¡æ•°
        success = cursor.execute(
            "SELECT COUNT(*) FROM analysis_history 
             WHERE user_id = ? AND status = 'success'",
            (user_id,)
        ).fetchone()[0]
        
        # å¤±è´¥æ¬¡æ•°
        failed = cursor.execute(
            "SELECT COUNT(*) FROM analysis_history 
             WHERE user_id = ? AND status = 'failed'",
            (user_id,)
        ).fetchone()[0]
        
        # å¹³å‡æ‰§è¡Œæ—¶é—´
        avg_time = cursor.execute(
            "SELECT AVG(execution_time) FROM analysis_history 
             WHERE user_id = ? AND status = 'success'",
            (user_id,)
        ).fetchone()[0] or 0
        
        # ... å…¶ä»–ç»Ÿè®¡
        
        return stats
```

---

## 4. æ•°æ®æŸ¥çœ‹æ–¹æ³•

### æ–¹æ³• 1ï¼šä½¿ç”¨ SQLite Browserï¼ˆæ¨èï¼‰

1. ä¸‹è½½ [DB Browser for SQLite](https://sqlitebrowser.org/)
2. æ‰“å¼€ `backend/feishu_app.db`
3. æŸ¥çœ‹è¡¨æ•°æ®

### æ–¹æ³• 2ï¼šå‘½ä»¤è¡ŒæŸ¥è¯¢

```bash
cd backend

# æŸ¥çœ‹ç”¨æˆ·åˆ—è¡¨
sqlite3 feishu_app.db "SELECT open_id, name, last_login_at FROM users;"

# æŸ¥çœ‹åˆ†æå†å²
sqlite3 feishu_app.db "SELECT user_id, query, status, created_at FROM analysis_history LIMIT 10;"

# æŸ¥çœ‹ç”¨æˆ·ç»Ÿè®¡
sqlite3 feishu_app.db "
SELECT 
    user_id,
    COUNT(*) as total_analyses,
    SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as success_count,
    SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed_count
FROM analysis_history 
GROUP BY user_id;
"

# æŸ¥çœ‹æ–‡ä»¶ä¸Šä¼ è®°å½•
sqlite3 feishu_app.db "SELECT user_id, file_name, file_size, created_at FROM file_uploads ORDER BY created_at DESC LIMIT 10;"
```

### æ–¹æ³• 3ï¼šPython è„šæœ¬æŸ¥è¯¢

```python
# backend/scripts/query_stats.py
import sqlite3

conn = sqlite3.connect('feishu_app.db')
cursor = conn.cursor()

# æŸ¥è¯¢æŸä¸ªç”¨æˆ·çš„ç»Ÿè®¡
user_id = "ou_xxx"
cursor.execute("""
    SELECT 
        COUNT(*) as total,
        SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as success,
        SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed,
        AVG(execution_time) as avg_time
    FROM analysis_history 
    WHERE user_id = ?
""", (user_id,))

stats = cursor.fetchone()
print(f"æ€»æ¬¡æ•°: {stats[0]}, æˆåŠŸ: {stats[1]}, å¤±è´¥: {stats[2]}, å¹³å‡æ—¶é—´: {stats[3]:.2f}ç§’")
```

---

## 5. æ•°æ®æŒä¹…åŒ–ä¿è¯

### âœ… å·²å®ç°
- ç”¨æˆ·ä¿¡æ¯æŒä¹…åŒ–ï¼ˆç™»å½•å³ä¿å­˜ï¼‰
- åˆ†æå†å²æŒä¹…åŒ–ï¼ˆæ¯æ¬¡åˆ†æåä¿å­˜ï¼‰
- æ–‡ä»¶ä¸Šä¼ è®°å½•æŒä¹…åŒ–
- ä¼šè¯çŠ¶æ€æŒä¹…åŒ–

### âœ… åˆ·æ–°ä¸ä¸¢å¤±
- Token å­˜å‚¨åœ¨ `localStorage`ï¼ˆæµè§ˆå™¨åˆ·æ–°ä¸ä¸¢å¤±ï¼‰
- æ•°æ®åº“å­˜å‚¨åœ¨æœ¬åœ°ç£ç›˜ï¼ˆæœåŠ¡é‡å¯ä¸ä¸¢å¤±ï¼‰
- æ–‡ä»¶å­˜å‚¨åœ¨ `uploads` ç›®å½•ï¼ˆæ°¸ä¹…ä¿å­˜ï¼‰

### âš ï¸ æ³¨æ„äº‹é¡¹

**å†…å­˜ç¼“å­˜æ•°æ®ï¼ˆä¼šä¸¢å¤±ï¼‰**ï¼š
```python
# backend/api/auth.py
login._token_cache[token] = user_info  # å†…å­˜ç¼“å­˜ï¼Œé‡å¯åä¸¢å¤±
```

**ç”Ÿäº§ç¯å¢ƒå»ºè®®**ï¼š
- ä½¿ç”¨ **Redis** å­˜å‚¨ token ç¼“å­˜
- å®šæœŸæ¸…ç†è¿‡æœŸæ–‡ä»¶å’Œæ•°æ®

---

## 6. æ•°æ®æ¸…ç†

### æ¸…ç†è¿‡æœŸ Tokenï¼ˆå†…å­˜ï¼‰
```python
# åœ¨ login å‡½æ•°ä¸­æ·»åŠ æ¸…ç†é€»è¾‘
import time

def clean_expired_tokens():
    if hasattr(login, '_token_cache'):
        now = time.time()
        expired = [
            token for token, data in login._token_cache.items()
            if now - data["login_time"] > 86400  # 24å°æ—¶
        ]
        for token in expired:
            del login._token_cache[token]
```

### æ¸…ç†è¿‡æœŸæ–‡ä»¶
```bash
# åˆ é™¤ 30 å¤©å‰çš„æ–‡ä»¶
find backend/uploads -type f -mtime +30 -delete
```

### æ¸…ç†è¿‡æœŸæ•°æ®
```sql
-- åˆ é™¤ 90 å¤©å‰çš„åˆ†æè®°å½•
DELETE FROM analysis_history 
WHERE created_at < datetime('now', '-90 days');

-- æ¸…ç†å·²åˆ é™¤çš„æ–‡ä»¶è®°å½•
DELETE FROM file_uploads 
WHERE is_deleted = 1 AND updated_at < datetime('now', '-30 days');
```

---

## 7. æ•°æ®å¤‡ä»½å»ºè®®

### è‡ªåŠ¨å¤‡ä»½è„šæœ¬
```bash
#!/bin/bash
# backup.sh

DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="./backups"

mkdir -p $BACKUP_DIR

# å¤‡ä»½æ•°æ®åº“
cp backend/feishu_app.db $BACKUP_DIR/feishu_app_$DATE.db
cp backend/data/analysis.db $BACKUP_DIR/analysis_$DATE.db

# å¤‡ä»½ä¸Šä¼ æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œæ–‡ä»¶è¾ƒå¤§ï¼‰
# tar -czf $BACKUP_DIR/uploads_$DATE.tar.gz backend/uploads/

# ä¿ç•™æœ€è¿‘ 7 å¤©çš„å¤‡ä»½
find $BACKUP_DIR -name "*.db" -mtime +7 -delete

echo "âœ… å¤‡ä»½å®Œæˆ: $DATE"
```

---

## ğŸ“Š æ€»ç»“

| æ•°æ®ç±»å‹ | å­˜å‚¨ä½ç½® | æŒä¹…åŒ– | å¤§å° |
|---------|----------|--------|------|
| **ä¸Šä¼ æ–‡ä»¶** | `backend/uploads/` | âœ… æ°¸ä¹… | ~100MB/æ–‡ä»¶ |
| **ç”¨æˆ·ä¿¡æ¯** | `feishu_app.db` â†’ users | âœ… æ°¸ä¹… | ~1KB/ç”¨æˆ· |
| **åˆ†æå†å²** | `feishu_app.db` â†’ analysis_history | âœ… æ°¸ä¹… | ~10KB/è®°å½• |
| **æ–‡ä»¶è®°å½•** | `feishu_app.db` â†’ file_uploads | âœ… æ°¸ä¹… | ~1KB/æ–‡ä»¶ |
| **ä¼šè¯çŠ¶æ€** | `feishu_app.db` â†’ user_sessions | âœ… æ°¸ä¹… | ~5KB/ä¼šè¯ |
| **Token ç¼“å­˜** | å†…å­˜ `_token_cache` | âŒ ä¸´æ—¶ | ~1KB/token |
| **æ–‡ä»¶ç¼“å­˜** | å†…å­˜ `file_cache` | âŒ ä¸´æ—¶ | ~1MB/æ–‡ä»¶ |

**æ‰€æœ‰é‡è¦æ•°æ®éƒ½æŒä¹…åŒ–å­˜å‚¨ï¼Œåˆ·æ–°æˆ–é‡å¯ä¸ä¼šä¸¢å¤±ï¼** âœ…

