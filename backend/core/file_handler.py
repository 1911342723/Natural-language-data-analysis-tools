"""
æ–‡ä»¶å¤„ç†æ¨¡å—
"""
import os
import uuid
import math
import pandas as pd
import numpy as np
import logging
from datetime import datetime, date, time
from typing import Dict, Any, List
from pathlib import Path

from config import settings

logger = logging.getLogger(__name__)

# é…ç½®å¸¸é‡
SAMPLE_SIZE = 5000  # é‡‡æ ·è¡Œæ•°ï¼ˆç”¨äºåˆ†æï¼‰
PREVIEW_SIZE = 100  # é¢„è§ˆè¡Œæ•°ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
LARGE_FILE_THRESHOLD = 50 * 1024 * 1024  # 50MBï¼Œè¶…è¿‡æ­¤å¤§å°ä½¿ç”¨é‡‡æ ·æ¨¡å¼


class FileHandler:
    """æ–‡ä»¶å¤„ç†å™¨"""
    
    @staticmethod
    async def save_uploaded_file(file_content: bytes, filename: str) -> str:
        """
        ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        
        Returns:
            file_id
        """
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶ID
        file_id = str(uuid.uuid4())
        file_ext = Path(filename).suffix
        
        # ä¿å­˜æ–‡ä»¶
        file_path = os.path.join(settings.upload_dir, f"{file_id}{file_ext}")
        
        with open(file_path, 'wb') as f:
            f.write(file_content)
        
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜: {file_path}")
        return file_id
    
    @staticmethod
    def _parse_dataframe(df: pd.DataFrame, sheet_name: str) -> Dict[str, Any]:
        """
        è§£æå•ä¸ª DataFrame
        
        Returns:
            {
                'sheet_name': str,
                'total_rows': int,
                'total_columns': int,
                'columns': [{name, type, nullable, stats}, ...],
                'preview': [...],
                'data_json': str
            }
        """
        total_rows, total_columns = df.shape
        
        # æå–åˆ—ä¿¡æ¯
        columns_info = []
        for col_name in df.columns:
            col_data = df[col_name]
            
            # æ•°æ®ç±»å‹
            dtype = str(col_data.dtype)
            if dtype.startswith('int'):
                col_type = 'int'
            elif dtype.startswith('float'):
                col_type = 'float'
            elif dtype == 'bool':
                col_type = 'bool'
            elif dtype == 'datetime64':
                col_type = 'datetime'
            else:
                col_type = 'string'
            
            # æ˜¯å¦å¯ç©º
            nullable = col_data.isnull().any()
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {}
            if col_type in ['int', 'float']:
                # å¤„ç†æ•°å€¼å‹å­—æ®µï¼Œå°† NaN è½¬æ¢ä¸º None
                def safe_float(value):
                    """å®‰å…¨è½¬æ¢ floatï¼ŒNaN è½¬ä¸º None"""
                    if pd.isna(value):
                        return None
                    try:
                        return float(value)
                    except:
                        return None
                
                # å…ˆå»é™¤ NaN å€¼ï¼Œé¿å…è­¦å‘Š
                valid_data = col_data.dropna()
                
                if len(valid_data) > 0:
                    stats = {
                        'min': safe_float(valid_data.min()),
                        'max': safe_float(valid_data.max()),
                        'mean': safe_float(valid_data.mean()),
                        'median': safe_float(valid_data.median()),
                        'std': safe_float(valid_data.std()),
                    }
                else:
                    # å¦‚æœåˆ—å…¨æ˜¯ NaNï¼Œç»Ÿè®¡å€¼éƒ½ä¸º None
                    stats = {
                        'min': None,
                        'max': None,
                        'mean': None,
                        'median': None,
                        'std': None,
                    }
            elif col_type == 'string':
                # å¤„ç†å­—ç¬¦ä¸²å‹å­—æ®µï¼Œè¿‡æ»¤æ‰ NaN
                unique_values = col_data.dropna().unique()
                # è½¬æ¢ä¸º Python åŸç”Ÿç±»å‹ï¼Œå¤„ç† NaN
                sample_values = []
                for val in unique_values[:5]:
                    if pd.notna(val):
                        sample_values.append(str(val))
                
                stats = {
                    'unique': len(unique_values),
                    'sample': sample_values,
                }
            
            columns_info.append({
                'name': col_name,
                'type': col_type,
                'nullable': bool(nullable),
                'stats': stats
            })
        
        # æ•°æ®é¢„è§ˆï¼ˆå‰100è¡Œï¼‰
        # å°† NaN æ›¿æ¢ä¸º Noneï¼Œä»¥ä¾¿ JSON åºåˆ—åŒ–
        preview_df = df.head(100)
        # ç›´æ¥è½¬æ¢ä¸ºå­—å…¸ï¼Œä¸éœ€è¦ fillna
        preview = preview_df.to_dict(orient='records')
        
        # é€’å½’æ¸…ç†æ‰€æœ‰ä¸å¯åºåˆ—åŒ–çš„å€¼
        def clean_nan(obj):
            """é€’å½’æ¸…ç†å¯¹è±¡ä¸­çš„æ‰€æœ‰ä¸å¯åºåˆ—åŒ–çš„å€¼ï¼ˆNaN, Timestamp, datetimeç­‰ï¼‰"""
            if isinstance(obj, dict):
                return {k: clean_nan(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [clean_nan(item) for item in obj]
            elif isinstance(obj, float) and (math.isnan(obj) or math.isinf(obj)):
                return None
            elif pd.isna(obj):
                # å¤„ç†æ‰€æœ‰ç±»å‹çš„ NaNï¼ˆåŒ…æ‹¬ pd.NaTï¼‰
                return None
            elif isinstance(obj, (pd.Timestamp, np.datetime64, datetime, date, time)):
                # å°†å„ç§ datetime ç±»å‹è½¬æ¢ä¸º ISO æ ¼å¼å­—ç¬¦ä¸²
                try:
                    return obj.isoformat()
                except:
                    return str(obj)
            elif hasattr(obj, 'item'):
                # NumPy ç±»å‹ï¼ˆå¦‚ np.int64ï¼‰è½¬æ¢ä¸º Python åŸç”Ÿç±»å‹
                return obj.item()
            else:
                return obj
        
        preview = clean_nan(preview)
        
        # å®Œæ•´æ•°æ®çš„ JSONï¼ˆç”¨äº Jupyter Kernelï¼‰
        # ä½¿ç”¨ pandas çš„ to_jsonï¼Œä¼šè‡ªåŠ¨å¤„ç† NaN
        data_json = df.to_json(orient='records', force_ascii=False, date_format='iso')
        
        return {
            'sheet_name': sheet_name,
            'total_rows': total_rows,
            'total_columns': total_columns,
            'columns': columns_info,
            'preview': preview,
            'data_json': data_json
        }
    
    @staticmethod
    def parse_file(file_id: str, filename: str) -> Dict[str, Any]:
        """
        è§£ææ–‡ä»¶å¹¶æå–ä¿¡æ¯ï¼ˆæ”¯æŒå¤šå·¥ä½œè¡¨ï¼‰
        
        Returns:
            {
                'file_id': str,
                'file_name': str,
                'file_size': int,
                'sheets': [
                    {
                        'sheet_name': str,
                        'total_rows': int,
                        'total_columns': int,
                        'columns': [{name, type, nullable, stats}, ...],
                        'preview': [...],
                        'data_json': str
                    },
                    ...
                ]
            }
        """
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        file_ext = Path(filename).suffix
        file_path = os.path.join(settings.upload_dir, f"{file_id}{file_ext}")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        logger.info(f"å¼€å§‹è§£ææ–‡ä»¶: {file_path}")
        
        file_size = os.path.getsize(file_path)
        sheets_data = []
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–
        if file_ext in ['.xlsx', '.xls']:
            # Excel æ–‡ä»¶ï¼šè¯»å–æ‰€æœ‰å·¥ä½œè¡¨
            excel_file = pd.ExcelFile(file_path)
            logger.info(f"Excel æ–‡ä»¶åŒ…å« {len(excel_file.sheet_names)} ä¸ªå·¥ä½œè¡¨: {excel_file.sheet_names}")
            
            for sheet_name in excel_file.sheet_names:
                # å…ˆè¯»å–å°æ ·æœ¬åˆ¤æ–­å¤§å°
                df_sample = pd.read_excel(file_path, sheet_name=sheet_name, nrows=1000)
                estimated_total_rows = len(df_sample)  # ä¸´æ—¶ä¼°ç®—
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºå¤§æ–‡ä»¶ï¼ˆExcel é€šå¸¸è¾ƒå°ï¼Œé˜ˆå€¼å¯ä»¥æ›´å®½æ¾ï¼‰
                if file_size > LARGE_FILE_THRESHOLD * 2:  # Excel é˜ˆå€¼ä¸º 100MB
                    logger.info(f"Excel å¤§æ–‡ä»¶æ£€æµ‹ï¼Œå·¥ä½œè¡¨ '{sheet_name}'")
                    # è¯»å–å…¨éƒ¨æ•°æ®åé‡‡æ ·
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    if len(df) > SAMPLE_SIZE:
                        logger.info(f"å·¥ä½œè¡¨ '{sheet_name}' è¿‡å¤§ï¼ˆ{len(df)} è¡Œï¼‰ï¼Œä½¿ç”¨é‡‡æ ·")
                        sheet_data = FileHandler._parse_large_dataframe_sampling(df, sheet_name)
                    else:
                        sheet_data = FileHandler._parse_dataframe(df, sheet_name)
                else:
                    # æ­£å¸¸å¤§å°ï¼Œå…¨é‡è¯»å–
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    logger.info(f"å·¥ä½œè¡¨ '{sheet_name}' æ•°æ®å½¢çŠ¶: {df.shape}")
                    sheet_data = FileHandler._parse_dataframe(df, sheet_name)
                
                sheets_data.append(sheet_data)
                
        elif file_ext == '.csv':
            # CSV æ–‡ä»¶ï¼šåªæœ‰ä¸€ä¸ªè¡¨
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨é‡‡æ ·æ¨¡å¼
            if file_size > LARGE_FILE_THRESHOLD:
                logger.info(f"å¤§æ–‡ä»¶æ£€æµ‹ï¼š{file_size / 1024 / 1024:.2f} MBï¼Œä½¿ç”¨é‡‡æ ·æ¨¡å¼")
                sheet_data = FileHandler._parse_large_csv_streaming(file_path, "Sheet1")
            else:
                # å°æ–‡ä»¶ï¼šå…¨é‡è¯»å–
                try:
                    df = pd.read_csv(file_path, encoding='utf-8')
                except:
                    try:
                        df = pd.read_csv(file_path, encoding='gbk')
                    except:
                        df = pd.read_csv(file_path, encoding='latin1')
                
                logger.info(f"CSV æ–‡ä»¶è§£ææˆåŠŸï¼Œæ•°æ®å½¢çŠ¶: {df.shape}")
                sheet_data = FileHandler._parse_dataframe(df, "Sheet1")
            
            sheets_data.append(sheet_data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
        
        result = {
            'file_id': file_id,
            'file_name': filename,
            'file_size': file_size,
            'sheets': sheets_data
        }
        
        logger.info(f"æ–‡ä»¶è§£æå®Œæˆï¼Œå…± {len(sheets_data)} ä¸ªå·¥ä½œè¡¨")
        return result
    
    @staticmethod
    def _parse_large_dataframe_sampling(df: pd.DataFrame, sheet_name: str) -> Dict[str, Any]:
        """
        å¯¹å·²åŠ è½½çš„å¤§ DataFrame è¿›è¡Œé‡‡æ ·ï¼ˆç”¨äº Excelï¼‰
        """
        total_rows = len(df)
        print(f"ğŸš€ [DataFrame é‡‡æ ·] å¼€å§‹å¤„ç†: {sheet_name}, æ€»è¡Œæ•°: {total_rows:,}")
        
        # éšæœºé‡‡æ ·
        if total_rows > SAMPLE_SIZE:
            df_sample = df.sample(n=SAMPLE_SIZE, random_state=42)
            print(f"ğŸ“Œ [DataFrame é‡‡æ ·] å·²é‡‡æ · {SAMPLE_SIZE:,} è¡Œ ({SAMPLE_SIZE/total_rows*100:.1f}%)")
        else:
            df_sample = df
        
        # ç”Ÿæˆåˆ—ä¿¡æ¯
        columns_info = []
        for col_name in df.columns:
            col_data = df[col_name]
            col_sample = df_sample[col_name]
            
            # æ•°æ®ç±»å‹
            dtype = str(col_data.dtype)
            if dtype.startswith('int'):
                col_type = 'int'
            elif dtype.startswith('float'):
                col_type = 'float'
            elif dtype == 'bool':
                col_type = 'bool'
            else:
                col_type = 'string'
            
            # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨å…¨é‡æ•°æ®ï¼‰
            stats = {}
            if col_type in ['int', 'float']:
                valid_data = col_data.dropna()
                if len(valid_data) > 0:
                    stats = {
                        'min': float(valid_data.min()) if not pd.isna(valid_data.min()) else None,
                        'max': float(valid_data.max()) if not pd.isna(valid_data.max()) else None,
                        'mean': float(valid_data.mean()) if not pd.isna(valid_data.mean()) else None,
                    }
            elif col_type == 'string':
                unique_vals = col_sample.dropna().unique()
                stats = {
                    'unique': len(unique_vals),
                    'sample': list(unique_vals[:5])
                }
            
            columns_info.append({
                'name': col_name,
                'type': col_type,
                'nullable': col_data.isnull().any(),
                'stats': stats
            })
        
        # ç”Ÿæˆé¢„è§ˆ
        preview_df = df_sample.head(PREVIEW_SIZE)
        preview = preview_df.to_dict(orient='records')
        
        # æ¸…ç†ä¸å¯åºåˆ—åŒ–çš„å€¼
        def clean_nan(obj):
            """é€’å½’æ¸…ç†å¯¹è±¡ä¸­çš„æ‰€æœ‰ä¸å¯åºåˆ—åŒ–çš„å€¼ï¼ˆNaN, Timestamp, datetimeç­‰ï¼‰"""
            if isinstance(obj, dict):
                return {k: clean_nan(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [clean_nan(item) for item in obj]
            elif isinstance(obj, float) and (math.isnan(obj) or math.isinf(obj)):
                return None
            elif pd.isna(obj):
                # å¤„ç†æ‰€æœ‰ç±»å‹çš„ NaNï¼ˆåŒ…æ‹¬ pd.NaTï¼‰
                return None
            elif isinstance(obj, (pd.Timestamp, np.datetime64, datetime, date, time)):
                # å°†å„ç§ datetime ç±»å‹è½¬æ¢ä¸º ISO æ ¼å¼å­—ç¬¦ä¸²
                try:
                    return obj.isoformat()
                except:
                    return str(obj)
            elif hasattr(obj, 'item'):
                return obj.item()
            else:
                return obj
        
        preview = clean_nan(preview)
        
        # data_json
        data_json = df_sample.to_json(orient='records', force_ascii=False, date_format='iso')
        
        print(f"âœ… [DataFrame é‡‡æ ·] å¤„ç†å®Œæˆ")
        
        return {
            'sheet_name': sheet_name,
            'total_rows': total_rows,
            'total_columns': len(df.columns),
            'columns': columns_info,
            'preview': preview,
            'data_json': data_json,
            'is_sampled': total_rows > SAMPLE_SIZE,
            'sample_size': len(df_sample)
        }
    
    @staticmethod
    def _parse_large_csv_streaming(file_path: str, sheet_name: str) -> Dict[str, Any]:
        """
        æµå¼è§£æå¤§ CSV æ–‡ä»¶ï¼ˆé‡‡æ ·æ¨¡å¼ï¼‰
        
        ç­–ç•¥ï¼š
        1. å¿«é€Ÿè®¡æ•°æ€»è¡Œæ•°
        2. éšæœºé‡‡æ · SAMPLE_SIZE è¡Œç”¨äºåˆ†æ
        3. æµå¼è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        """
        print(f"ğŸš€ [å¤§æ–‡ä»¶å¤„ç†] å¼€å§‹æµå¼è§£æ: {file_path}")
        
        # ç¬¬1æ­¥ï¼šå¿«é€Ÿè·å–æ€»è¡Œæ•°å’Œåˆ—å
        with open(file_path, 'r', encoding='utf-8') as f:
            # è¯»å–è¡¨å¤´
            header = f.readline().strip().split(',')
            # å¿«é€Ÿè®¡æ•°
            total_rows = sum(1 for _ in f)
        
        print(f"ğŸ“Š [å¤§æ–‡ä»¶å¤„ç†] æ€»è¡Œæ•°: {total_rows:,}, åˆ—æ•°: {len(header)}")
        
        # ç¬¬2æ­¥ï¼šæ™ºèƒ½é‡‡æ ·ï¼ˆå¦‚æœæ•°æ®é‡å¤ªå¤§ï¼‰
        if total_rows > SAMPLE_SIZE:
            # è®¡ç®—é‡‡æ ·ç‡
            skip_prob = 1 - (SAMPLE_SIZE / total_rows)
            print(f"ğŸ“Œ [å¤§æ–‡ä»¶å¤„ç†] é‡‡æ ·æ¨¡å¼ï¼šä¿ç•™ {SAMPLE_SIZE:,} è¡Œ ({SAMPLE_SIZE/total_rows*100:.1f}%)")
            
            # éšæœºé‡‡æ ·
            df_sample = pd.read_csv(
                file_path,
                skiprows=lambda i: i > 0 and np.random.random() < skip_prob
            )
        else:
            # æ•°æ®é‡é€‚ä¸­ï¼Œå…¨é‡è¯»å–
            df_sample = pd.read_csv(file_path)
        
        print(f"âœ… [å¤§æ–‡ä»¶å¤„ç†] é‡‡æ ·å®Œæˆï¼š{len(df_sample)} è¡Œ")
        
        # ç¬¬3æ­¥ï¼šæµå¼è®¡ç®—ç²¾ç¡®ç»Ÿè®¡ï¼ˆéå†æ‰€æœ‰æ•°æ®ï¼‰
        print(f"ğŸ“ˆ [å¤§æ–‡ä»¶å¤„ç†] å¼€å§‹æµå¼ç»Ÿè®¡è®¡ç®—...")
        streaming_stats = FileHandler._calculate_streaming_stats(file_path)
        
        # ç¬¬4æ­¥ï¼šä½¿ç”¨é‡‡æ ·æ•°æ®ç”Ÿæˆåˆ—ä¿¡æ¯ï¼ˆç»“åˆæµå¼ç»Ÿè®¡ï¼‰
        columns_info = []
        for col_name in df_sample.columns:
            col_data = df_sample[col_name]
            
            # æ•°æ®ç±»å‹
            dtype = str(col_data.dtype)
            if dtype.startswith('int'):
                col_type = 'int'
            elif dtype.startswith('float'):
                col_type = 'float'
            elif dtype == 'bool':
                col_type = 'bool'
            else:
                col_type = 'string'
            
            # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¼˜å…ˆä½¿ç”¨æµå¼ç»Ÿè®¡çš„ç²¾ç¡®å€¼ï¼‰
            stats = {}
            if col_name in streaming_stats:
                stats = streaming_stats[col_name]
            elif col_type in ['int', 'float']:
                valid_data = col_data.dropna()
                if len(valid_data) > 0:
                    stats = {
                        'min': float(valid_data.min()) if not pd.isna(valid_data.min()) else None,
                        'max': float(valid_data.max()) if not pd.isna(valid_data.max()) else None,
                        'mean': float(valid_data.mean()) if not pd.isna(valid_data.mean()) else None,
                    }
            elif col_type == 'string':
                unique_vals = col_data.dropna().unique()
                stats = {
                    'unique': len(unique_vals),
                    'sample': list(unique_vals[:5])
                }
            
            columns_info.append({
                'name': col_name,
                'type': col_type,
                'nullable': col_data.isnull().any(),
                'stats': stats
            })
        
        # ç¬¬5æ­¥ï¼šç”Ÿæˆé¢„è§ˆå’Œæ•°æ® JSONï¼ˆåªç”¨é‡‡æ ·æ•°æ®ï¼‰
        preview_df = df_sample.head(PREVIEW_SIZE)
        preview = preview_df.to_dict(orient='records')
        
        # æ¸…ç†ä¸å¯åºåˆ—åŒ–çš„å€¼
        def clean_nan(obj):
            """é€’å½’æ¸…ç†å¯¹è±¡ä¸­çš„æ‰€æœ‰ä¸å¯åºåˆ—åŒ–çš„å€¼ï¼ˆNaN, Timestamp, datetimeç­‰ï¼‰"""
            if isinstance(obj, dict):
                return {k: clean_nan(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [clean_nan(item) for item in obj]
            elif isinstance(obj, float) and (math.isnan(obj) or math.isinf(obj)):
                return None
            elif pd.isna(obj):
                # å¤„ç†æ‰€æœ‰ç±»å‹çš„ NaNï¼ˆåŒ…æ‹¬ pd.NaTï¼‰
                return None
            elif isinstance(obj, (pd.Timestamp, np.datetime64, datetime, date, time)):
                # å°†å„ç§ datetime ç±»å‹è½¬æ¢ä¸º ISO æ ¼å¼å­—ç¬¦ä¸²
                try:
                    return obj.isoformat()
                except:
                    return str(obj)
            elif hasattr(obj, 'item'):
                return obj.item()
            else:
                return obj
        
        preview = clean_nan(preview)
        
        # data_json åªä¿å­˜é‡‡æ ·æ•°æ®ï¼ˆç”¨äº Jupyter åˆ†æï¼‰
        data_json = df_sample.to_json(orient='records', force_ascii=False, date_format='iso')
        
        print(f"âœ… [å¤§æ–‡ä»¶å¤„ç†] è§£æå®Œæˆ")
        
        return {
            'sheet_name': sheet_name,
            'total_rows': total_rows,
            'total_columns': len(header),
            'columns': columns_info,
            'preview': preview,
            'data_json': data_json,
            'is_sampled': total_rows > SAMPLE_SIZE,  # æ ‡è®°æ˜¯å¦é‡‡æ ·
            'sample_size': len(df_sample)  # å®é™…é‡‡æ ·è¡Œæ•°
        }
    
    @staticmethod
    def _calculate_streaming_stats(file_path: str, chunk_size: int = 10000) -> Dict[str, Dict]:
        """æµå¼è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸å ç”¨å¤§é‡å†…å­˜ï¼‰"""
        stats = {}
        
        try:
            for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):
                for col in chunk.columns:
                    if col not in stats:
                        stats[col] = {
                            'min': float('inf'),
                            'max': float('-inf'),
                            'sum': 0,
                            'count': 0
                        }
                    
                    if pd.api.types.is_numeric_dtype(chunk[col]):
                        valid_data = chunk[col].dropna()
                        if len(valid_data) > 0:
                            stats[col]['min'] = min(stats[col]['min'], valid_data.min())
                            stats[col]['max'] = max(stats[col]['max'], valid_data.max())
                            stats[col]['sum'] += valid_data.sum()
                            stats[col]['count'] += len(valid_data)
                
                # æ¯å¤„ç†10ä¸ªchunkæ‰“å°ä¸€æ¬¡è¿›åº¦
                if i % 10 == 0 and i > 0:
                    print(f"ğŸ“Š [æµå¼ç»Ÿè®¡] å·²å¤„ç† {(i+1)*chunk_size:,} è¡Œ...")
        
        except Exception as e:
            print(f"âš ï¸ [æµå¼ç»Ÿè®¡] è­¦å‘Š: {e}ï¼Œè·³è¿‡æµå¼ç»Ÿè®¡")
            return {}
        
        # è®¡ç®—å¹³å‡å€¼
        for col, col_stats in stats.items():
            if col_stats['count'] > 0:
                col_stats['mean'] = col_stats['sum'] / col_stats['count']
                # æ¸…ç†æ— ç©·å¤§å€¼
                if math.isinf(col_stats['min']):
                    col_stats['min'] = None
                if math.isinf(col_stats['max']):
                    col_stats['max'] = None
                del col_stats['sum']  # åˆ é™¤ä¸­é—´å˜é‡
        
        return stats
    
    @staticmethod
    def get_file_path(file_id: str, filename: str) -> str:
        """è·å–æ–‡ä»¶è·¯å¾„"""
        file_ext = Path(filename).suffix
        return os.path.join(settings.upload_dir, f"{file_id}{file_ext}")


# å…¨å±€å®ä¾‹
file_handler = FileHandler()

