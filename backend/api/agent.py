"""
Agent åˆ†æ API
"""
from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import List, Dict, Any, AsyncGenerator
import asyncio
import uuid
import json
import logging

from core.agent import AnalysisAgent
from core.file_handler import file_handler

logger = logging.getLogger(__name__)

router = APIRouter()

# å…¨å±€ä»»åŠ¡å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redisï¼‰
tasks: Dict[str, Dict[str, Any]] = {}


class AnalyzeRequest(BaseModel):
    """åˆ†æè¯·æ±‚"""
    session_id: str
    user_request: str
    selected_columns: List[str]


async def run_agent_task(
    task_id: str,
    session_id: str,
    user_request: str,
    selected_columns: List[str],
    data_schema: Dict
):
    """åå°è¿è¡Œ Agent ä»»åŠ¡"""
    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œ Agent ä»»åŠ¡: {task_id}")
        
        # åˆ›å»º Agent
        agent = AnalysisAgent(
            session_id=session_id,
            user_request=user_request,
            selected_columns=selected_columns,
            data_schema=data_schema
        )
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        tasks[task_id]["agent"] = agent
        tasks[task_id]["status"] = "running"
        
        # æ‰§è¡Œ Agent
        result = await agent.run()
        
        # æ›´æ–°ä»»åŠ¡ç»“æœ
        tasks[task_id]["status"] = result["status"]
        tasks[task_id]["result"] = result
        
        logger.info(f"Agent ä»»åŠ¡å®Œæˆ: {task_id}, status={result['status']}")
    
    except Exception as e:
        logger.error(f"Agent ä»»åŠ¡å¼‚å¸¸: {e}", exc_info=True)
        tasks[task_id]["status"] = "failed"
        tasks[task_id]["result"] = {
            "status": "failed",
            "data": {
                "error": str(e)
            }
        }


@router.post("/agent/analyze")
async def submit_analysis(
    request: AnalyzeRequest,
    background_tasks: BackgroundTasks
):
    """
    æäº¤åˆ†æè¯·æ±‚
    
    è¯·æ±‚ï¼š
    {
        "session_id": "xxx",
        "user_request": "è®¡ç®—é”€å”®é¢å¹³å‡å€¼",
        "selected_columns": ["é”€å”®é¢", "åœ°åŒº"]
    }
    
    è¿”å›ï¼š
    {
        "success": true,
        "message": "ä»»åŠ¡å·²æäº¤",
        "data": {
            "task_id": "xxx"
        }
    }
    """
    try:
        # ç”Ÿæˆä»»åŠ¡ ID
        task_id = str(uuid.uuid4())
        
        logger.info(f"æ¥æ”¶åˆ†æè¯·æ±‚: task_id={task_id}, session={request.session_id}")
        
        # ä»ç¼“å­˜ä¸­è·å– Session ä¿¡æ¯
        from core.cache import session_cache, file_cache
        
        session_info = session_cache.get(request.session_id)
        if not session_info:
            raise HTTPException(status_code=404, detail="Session ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°åˆ›å»º")
        
        # ä»ç¼“å­˜ä¸­è·å–æ–‡ä»¶ä¿¡æ¯
        file_info = file_cache.get(session_info['file_id'])
        if not file_info:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¿¡æ¯ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
        
        # ä» session ä¸­è·å–å·¥ä½œè¡¨åç§°
        sheet_name = session_info.get('sheet_name')
        if not sheet_name:
            raise HTTPException(status_code=400, detail="Session ä¸­æ²¡æœ‰å·¥ä½œè¡¨ä¿¡æ¯")
        
        # æ‰¾åˆ°å¯¹åº”çš„å·¥ä½œè¡¨
        target_sheet = None
        for sheet in file_info['sheets']:
            if sheet['sheet_name'] == sheet_name:
                target_sheet = sheet
                break
        
        if not target_sheet:
            raise HTTPException(status_code=404, detail=f"å·¥ä½œè¡¨ '{sheet_name}' ä¸å­˜åœ¨")
        
        # æ„å»º data_schemaï¼ˆä½¿ç”¨å·¥ä½œè¡¨çš„ä¿¡æ¯ï¼‰
        data_schema = {
            "sheet_name": sheet_name,
            "total_rows": target_sheet['total_rows'],
            "total_columns": target_sheet['total_columns'],
            "columns": {col['name']: col for col in target_sheet['columns']}
        }
        
        logger.info(f"âœ… ä»ç¼“å­˜è·å–æ•°æ®ä¿¡æ¯æˆåŠŸ: å·¥ä½œè¡¨={sheet_name}, è¡Œæ•°={target_sheet['total_rows']}")
        
        # åˆå§‹åŒ–ä»»åŠ¡
        tasks[task_id] = {
            "task_id": task_id,
            "session_id": request.session_id,
            "status": "pending",
            "agent": None,
            "result": None
        }
        
        # åå°æ‰§è¡Œ Agent
        background_tasks.add_task(
            run_agent_task,
            task_id,
            request.session_id,
            request.user_request,
            request.selected_columns,
            data_schema
        )
        
        return JSONResponse({
            "success": True,
            "message": "ä»»åŠ¡å·²æäº¤",
            "data": {
                "task_id": task_id
            }
        })
    
    except Exception as e:
        logger.error(f"æäº¤åˆ†æè¯·æ±‚å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/agent/status/{task_id}")
async def get_agent_status(task_id: str):
    """
    è·å– Agent æ‰§è¡ŒçŠ¶æ€ï¼ˆè½®è¯¢ï¼‰
    
    è¿”å›ï¼š
    {
        "success": true,
        "status": "running",  # pending | running | completed | failed
        "data": {
            "steps": [...],
            "result": {...}
        }
    }
    """
    task = tasks.get(task_id)
    
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    # è·å–æœ€æ–°çŠ¶æ€
    agent = task.get("agent")
    if agent:
        state = agent.get_state()
        return JSONResponse({
            "success": True,
            "status": state["status"],
            "data": state["data"]
        })
    else:
        return JSONResponse({
            "success": True,
            "status": task["status"],
            "data": {
                "steps": [],
                "result": task.get("result")
            }
        })


@router.post("/agent/stop/{task_id}")
async def stop_agent(task_id: str):
    """åœæ­¢ Agent æ‰§è¡Œ"""
    task = tasks.get(task_id)
    
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    # TODO: å®ç°åœæ­¢é€»è¾‘
    task["status"] = "cancelled"
    
    return JSONResponse({
        "success": True,
        "message": "ä»»åŠ¡å·²åœæ­¢"
    })


@router.post("/agent/analyze-stream")
async def analyze_stream(request: AnalyzeRequest):
    """
    æµå¼åˆ†æï¼ˆSSEï¼‰- å®æ—¶æ¨é€ Agent æ‰§è¡ŒçŠ¶æ€
    
    è¿”å› SSE æµï¼Œå®¢æˆ·ç«¯å¯å®æ—¶æ¥æ”¶ï¼š
    - ä»£ç ç”Ÿæˆè¿›åº¦
    - ä»£ç æ‰§è¡Œè¾“å‡º
    - ç»“æœæå–
    - AI æ€»ç»“
    """
    try:
        logger.info(f"æ¥æ”¶æµå¼åˆ†æè¯·æ±‚: session={request.session_id}")
        
        # ä»ç¼“å­˜ä¸­è·å– Session ä¿¡æ¯
        from core.cache import session_cache, file_cache
        
        session_info = session_cache.get(request.session_id)
        if not session_info:
            raise HTTPException(status_code=404, detail="Session ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°åˆ›å»º")
        
        # åˆ¤æ–­æ˜¯å•æ–‡ä»¶è¿˜æ˜¯å¤šæ–‡ä»¶æ¨¡å¼
        is_multi = session_info.get('is_multi', False)
        
        if is_multi:
            # å¤šæ–‡ä»¶æ¨¡å¼ï¼šæ„å»ºå¤šè¡¨æ ¼çš„ data_schema
            logger.info(f"å¤šæ–‡ä»¶æ¨¡å¼åˆ†æ: group_id={session_info.get('group_id')}")
            
            tables_info = []
            for table in session_info.get('tables', []):
                # ä»ç¼“å­˜è·å–æ–‡ä»¶ä¿¡æ¯
                file_info = file_cache.get(table['file_id'])
                if file_info:
                    # æ‰¾åˆ°å¯¹åº”çš„å·¥ä½œè¡¨
                    for sheet in file_info['sheets']:
                        if sheet['sheet_name'] == table['sheet_name']:
                            # ä» table ä¸­è·å–ç”¨æˆ·é€‰æ‹©çš„å­—æ®µ
                            selected_columns = table.get('selected_columns', [])
                            
                            tables_info.append({
                                'alias': table['alias'],
                                'file_name': table['file_name'],
                                'sheet_name': table['sheet_name'],
                                'total_rows': sheet['total_rows'],
                                'total_columns': sheet['total_columns'],
                                'columns': sheet['columns'],
                                'selected_columns': selected_columns  # æ·»åŠ ç”¨æˆ·é€‰æ‹©çš„å­—æ®µ
                            })
                            
                            logger.info(f"  - è¡¨æ ¼ {table['alias']}: {len(selected_columns)} ä¸ªé€‰ä¸­å­—æ®µ")
                            break
            
            data_schema = {
                'is_multi': True,
                'tables': tables_info
            }
            logger.info(f"âœ… å¤šè¡¨æ ¼ data_schema æ„å»ºå®Œæˆ: {len(tables_info)} ä¸ªè¡¨æ ¼")
        else:
            # å•æ–‡ä»¶æ¨¡å¼ï¼šåŸæœ‰é€»è¾‘
            file_info = file_cache.get(session_info['file_id'])
            if not file_info:
                raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¿¡æ¯ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
            
            # ä» session ä¸­è·å–å·¥ä½œè¡¨åç§°
            sheet_name = session_info.get('sheet_name')
            if not sheet_name:
                raise HTTPException(status_code=400, detail="Session ä¸­æ²¡æœ‰å·¥ä½œè¡¨ä¿¡æ¯")
            
            # æ‰¾åˆ°å¯¹åº”çš„å·¥ä½œè¡¨
            target_sheet = None
            for sheet in file_info['sheets']:
                if sheet['sheet_name'] == sheet_name:
                    target_sheet = sheet
                    break
            
            if not target_sheet:
                raise HTTPException(status_code=404, detail=f"å·¥ä½œè¡¨ '{sheet_name}' ä¸å­˜åœ¨")
            
            # æ„å»º data_schema
            data_schema = {
                "sheet_name": sheet_name,
                "total_rows": target_sheet['total_rows'],
                "total_columns": target_sheet['total_columns'],
                "columns": {col['name']: col for col in target_sheet['columns']}
            }
        
        # ç”Ÿæˆä»»åŠ¡ ID
        task_id = str(uuid.uuid4())
        logger.info(f"åˆ›å»ºæµå¼ä»»åŠ¡: {task_id}")
        
        # åˆ›å»ºæµå¼å“åº”
        def safe_json_dumps(data: dict) -> str:
            """å®‰å…¨çš„ JSON åºåˆ—åŒ–ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦"""
            try:
                return json.dumps(data, ensure_ascii=False, default=str)
            except Exception as e:
                logger.error(f"JSON åºåˆ—åŒ–å¤±è´¥: {e}")
                # å°è¯•ç®€åŒ–æ•°æ®
                simplified = {
                    'event': data.get('event', 'error'),
                    'message': 'JSON åºåˆ—åŒ–å¤±è´¥ï¼Œæ•°æ®å·²ç®€åŒ–'
                }
                return json.dumps(simplified, ensure_ascii=False)
        
        async def event_generator() -> AsyncGenerator[str, None]:
            """SSE äº‹ä»¶ç”Ÿæˆå™¨"""
            try:
                # å‘é€ä»»åŠ¡å¼€å§‹äº‹ä»¶
                start_event = safe_json_dumps({'event': 'start', 'task_id': task_id})
                yield f"data: {start_event}\n\n"
                
                # åˆ›å»º Agent
                agent = AnalysisAgent(
                    session_id=request.session_id,
                    user_request=request.user_request,
                    selected_columns=request.selected_columns,
                    data_schema=data_schema
                )
                
                # ç›‘å¬ Agent çš„æ­¥éª¤å˜åŒ–
                last_step_count = 0
                last_step_outputs = {}  # è®°å½•æ¯ä¸ªæ­¥éª¤çš„æœ€åè¾“å‡ºï¼Œç”¨äºæ£€æµ‹å˜åŒ–
                
                # å¯åŠ¨ Agent ä»»åŠ¡
                agent_task = asyncio.create_task(agent.run())
                
                try:
                    # è½®è¯¢ Agent çŠ¶æ€å¹¶æ¨é€
                    while not agent_task.done():
                        state = agent.get_state()
                        current_steps = state['data']['steps']
                        current_step_count = len(current_steps)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ­¥éª¤æˆ–æ­¥éª¤å†…å®¹å˜åŒ–
                        for i, step in enumerate(current_steps):
                            step_key = f"{i}"
                            current_output = step.get('output', '')
                            
                            # æ–°æ­¥éª¤æˆ–è¾“å‡ºå†…å®¹å˜åŒ–
                            if i >= last_step_count or last_step_outputs.get(step_key) != current_output:
                                # é™åˆ¶è¾“å‡ºé•¿åº¦ï¼Œé¿å…æ•°æ®è¿‡å¤§
                                step_copy = step.copy()
                                if step_copy.get('output') and len(step_copy['output']) > 10000:
                                    step_copy['output'] = step_copy['output'][:10000] + '\n... (è¾“å‡ºè¿‡é•¿ï¼Œå·²æˆªæ–­)'
                                
                                # é™åˆ¶ä»£ç é•¿åº¦
                                if step_copy.get('code') and len(step_copy['code']) > 50000:
                                    step_copy['code'] = step_copy['code'][:50000] + '\n# ... (ä»£ç è¿‡é•¿ï¼Œå·²æˆªæ–­)'
                                
                                # è°ƒè¯•è¾“å‡º
                                logger.info(f"ğŸ“¤ æ¨é€æ­¥éª¤æ›´æ–° #{i}: {step.get('title')}, status={step.get('status')}, output_len={len(current_output)}")
                                
                                step_event = safe_json_dumps({'event': 'step', 'data': step_copy, 'step_index': i})
                                yield f"data: {step_event}\n\n"
                                
                                # æ›´æ–°è®°å½•
                                last_step_outputs[step_key] = current_output
                        
                        last_step_count = current_step_count
                        await asyncio.sleep(0.03)  # æ¯30msæ£€æŸ¥ä¸€æ¬¡ï¼ˆæ›´å®æ—¶ï¼‰
                
                except (asyncio.CancelledError, GeneratorExit) as e:
                    # å®¢æˆ·ç«¯æ–­å¼€è¿æ¥æˆ–å–æ¶ˆè¯·æ±‚
                    logger.info(f"âš ï¸ å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ï¼Œå–æ¶ˆä»»åŠ¡: {task_id}")
                    agent_task.cancel()
                    try:
                        await agent_task
                    except asyncio.CancelledError:
                        logger.info(f"âœ… Agent ä»»åŠ¡å·²æˆåŠŸå–æ¶ˆ: {task_id}")
                    raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œç»“æŸç”Ÿæˆå™¨
                
                # Agent æ‰§è¡Œå®Œæˆ
                result = await agent_task
                
                # é™åˆ¶ç»“æœæ•°æ®å¤§å°
                if result.get('data', {}).get('result'):
                    result_data = result['data']['result']
                    # é™åˆ¶æ–‡æœ¬è¾“å‡º
                    if 'text' in result_data and isinstance(result_data['text'], list):
                        result_data['text'] = [
                            t[:5000] + '...(å·²æˆªæ–­)' if len(t) > 5000 else t
                            for t in result_data['text']
                        ]
                
                # æ¨é€å®Œæˆäº‹ä»¶
                complete_event = safe_json_dumps({'event': 'complete', 'data': result})
                yield f"data: {complete_event}\n\n"
                
                logger.info(f"æµå¼ä»»åŠ¡å®Œæˆ: {task_id}")
                
            except Exception as e:
                logger.error(f"æµå¼ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
                error_event = safe_json_dumps({'event': 'error', 'message': str(e)})
                yield f"data: {error_event}\n\n"
        
        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"  # ç¦ç”¨ Nginx ç¼“å†²
            }
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æµå¼åˆ†æè¯·æ±‚å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

